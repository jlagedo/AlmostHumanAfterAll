# Apple Intelligence's on-device foundation model: a complete technical guide

Apple's on-device foundation model is a **~3.18 billion parameter** dense decoder-only transformer that runs entirely on iPhone, iPad, and Mac hardware — delivering summarization, text generation, Smart Reply, content classification, and structured data extraction without any data leaving the device. First shipped in iOS 18.1 (October 2024) and substantially redesigned for iOS 26 (2025), this model represents one of the most aggressively optimized large language models ever deployed on consumer mobile hardware, compressed to just **2 bits per weight** while maintaining competitive quality against models twice its size. At WWDC 2025, Apple opened direct developer access through the **Foundation Models framework**, enabling third-party apps to run inference against the on-device model with as few as three lines of Swift code.

---

## Architecture: a dual-block transformer compressed to fit in your pocket

The on-device model has gone through two generations, each reflecting distinct architectural philosophies. The 2024 release used a conventional 26-layer transformer with 3,072-dimensional hidden states and ~3B parameters (2.58B non-embedding, 0.15B shared input/output embeddings), created by pruning a 6.4B-parameter model and distilling from it. The 2025 redesign is substantially different: **56 layers** split into two segments (35 + 21), a 2,048-dimensional hidden size, 16 attention heads per layer, and a 6,656-dimensional feed-forward network using a 3.25× expansion ratio.

Both generations share core design choices drawn from modern efficient transformer research: **Grouped-Query Attention** with 8 key-value heads (reducing KV-cache memory), **RMSNorm** pre-normalization, **SwiGLU** activation functions, **Rotary Positional Embeddings** (RoPE) with a base frequency of 500,000, and shared input/output embedding matrices.

The 2025 model's most significant architectural innovation is **KV-cache sharing via a dual-block design**. The model splits into Block 1 (35 layers) and Block 2 (21 layers) at a 5:3 ratio. Block 2's layers have their key and value projections removed entirely — instead, all 21 layers in Block 2 share the KV-cache generated by Block 1's final layer. This eliminates **37.5% of KV-cache memory** and dramatically reduces time-to-first-token latency because Block 2 bypasses prefill computation. The 2025 model also introduces an interleaved attention architecture alternating between sliding-window local attention layers (with RoPE) and global attention layers without positional embeddings, improving length generalization.

A companion **48.77 million parameter draft model** (12-layer uniform transformer, 256 hidden dimensions) enables speculative decoding. It generates 4–8 candidate tokens rapidly; the full 3.18B model verifies candidates in a single forward pass. Typical acceptance rates of 60–80% yield a **2–4× inference speedup**. The draft model adds only ~186 MB of memory overhead.

For visual understanding, the 2025 model adds a **ViTDet-L vision encoder** with approximately 300 million parameters, augmented with a Register-Window mechanism for capturing both global context and local details. This powers on-device Visual Intelligence features like calendar event creation from photographed flyers.

### Vocabulary and context window

The tokenizer expanded dramatically between generations. The 2024 model used a **49,000-token** BPE vocabulary (via SentencePiece), while the 2025 model expanded to **153,600 tokens** to support 16 languages with only ~25% more tokens per input. The context window grew from **32,768 tokens** (2024, trained in stages from 4K → 8K → 32K) to a theoretical maximum of ~205K tokens via RoPE scaling, with practical high-quality range around **65K–100K tokens** in the model itself. However, the developer-facing Foundation Models API enforces a hard **4,096-token context window** (combined input and output).

---

## Quantization pushes a 3B model below 1 GB

The on-device model's memory efficiency stems from aggressive compression. The 2024 release used **mixed 2-bit and 4-bit palettization** averaging 3.7 bits per weight, guided by Apple's internal Talaria tool for per-operation bit-rate selection based on latency and power analysis. The 2025 model goes further with **2-bit Quantization-Aware Training (QAT)** — a technique where quantization is embedded directly in the training loop rather than applied post-hoc.

Key compression details for the 2025 model include a balanced 2-bit weight set ({-1.5, -0.5, 0.5, 1.5}) that enables smoother training with fewer loss spikes, **4-bit embeddings** jointly trained with base weights during QAT, and **8-bit KV-cache** quantization. LoRA adapters trained post-compression recover quality lost during quantization — a technique Apple calls "accuracy-recovery adapters," which then serve as initialization points for task-specific fine-tuning.

The production memory footprint lands at approximately **1.0–1.1 GB** for the compressed decoder weights (down from ~12.1 GB at FP32). Quality impact is modest: roughly 4.6% regression on the MGSM multilingual math benchmark and a slight 1.5% improvement on MMLU. In human evaluations, the 2024 3B model outperformed Phi-3-mini, Mistral-7B, Gemma-7B, and Llama-3-8B despite being significantly smaller. The 2025 model performs "favorably against Qwen-2.5-3B and Gemma-3n" and is "competitive against the larger Qwen-3-4B and Gemma-3-4B."

| Component | 2024 | 2025 |
|-----------|------|------|
| Decoder weights | Mixed 2/4-bit (~3.7 bpw) | 2-bit via QAT |
| Embeddings | Quantized (unspecified) | 4-bit via QAT |
| KV-cache | Quantized (unspecified) | 8-bit |
| Total footprint | ~1.5 GB estimated | ~1.0–1.1 GB |
| Adapter recovery | LoRA rank-16 | LoRA rank-32 |

---

## LoRA adapters make one model serve dozens of tasks

Rather than shipping separate models for each feature, Apple uses **LoRA (Low-Rank Adaptation) adapters** as lightweight neural network modules plugged into the frozen base model. Each adapter specializes the general-purpose model for a specific task — summarization, Smart Reply, mail categorization, Visual Intelligence event extraction, and more. Adapters are applied to all linear projection matrices in self-attention layers and all fully connected layers in feed-forward networks.

The 2024 adapters used **rank 16** with 16-bit precision, requiring only tens of megabytes of storage each. The 2025 generation increased to **rank 32** adapters (~150–160 MB each). Critically, adapters are **dynamically loaded, cached, and swapped at runtime** — the system loads only the adapter needed for the current task and releases it when done, managing overall memory pressure efficiently.

Apple also exposes adapter training to developers. A Python toolkit supports training custom rank-32 adapters with specific LoRA configuration (rank 32, alpha 16, scaling 0.5, dropout 0.0). Trained adapters are packaged as `.fmadapter` files and distributed via the Background Assets framework. One important caveat: **adapters are version-specific** and must be retrained whenever Apple updates the base model with an OS release.

---

## The Foundation Models framework gives developers direct model access

Before WWDC 2025, developers could only access Apple Intelligence through system-level APIs — Writing Tools, Image Playground, Genmoji, and App Intents for Siri. The **Foundation Models framework**, announced June 9, 2025 and shipping with iOS 26, iPadOS 26, and macOS Tahoe 26, changed this by opening direct on-device inference to third-party apps. Key characteristics: on-device only, free of cost, no API keys needed, offline-capable, zero app size impact (the model is built into the OS), and deeply integrated with Swift's type system.

### Core API: LanguageModelSession

The primary interface requires just three lines of code for basic text generation:

```swift
import FoundationModels

let session = LanguageModelSession()
let response = try await session.respond(to: "Summarize this article about climate change.")
print(response.content)
```

Sessions support custom system instructions, tool attachment, multi-turn conversation via a `transcript` property, streaming via `streamResponse(to:generating:)`, and availability checking through `SystemLanguageModel.default.availability`.

### Guided generation with @Generable

The framework's most distinctive feature is **constrained decoding** mapped to Swift types via the `@Generable` macro. This guarantees structurally correct, typed output — not raw text that must be parsed:

```swift
@Generable
struct RecipeExtraction {
    @Guide(description: "The recipe name")
    var name: String
    @Guide(description: "List of ingredients", .maximumCount(20))
    var ingredients: [String]
    @Guide(description: "Cooking time in minutes")
    var cookingTime: Int
}

let response = try await session.respond(
    to: "Extract the recipe from this text: ...",
    generating: RecipeExtraction.self
)
// response.content is a fully typed RecipeExtraction instance
```

Supported types include String, Int, Double, Float, Bool, arrays, composed structs, enums, and recursive types. The `@Guide` macro provides fine-grained control via `.count()`, `.maximumCount()`, regex patterns, and natural-language description parameters. Properties are generated in declaration order — Apple recommends placing summaries last for best quality.

### Tool calling protocol

Developers define tools the model can autonomously invoke to fetch external data or perform actions:

```swift
struct GetWeatherTool: Tool {
    let name = "getWeather"
    let description = "Retrieve the latest weather for a city"
    
    @Generable struct Arguments {
        @Guide(description: "The city name") var city: String
    }
    
    func call(arguments: Arguments) async throws -> ToolOutput {
        let weather = try await WeatherService.shared.weather(for: city)
        return ToolOutput(/* formatted weather data */)
    }
}
```

### Streaming and SwiftUI integration

Instead of raw token deltas, the framework streams **snapshots** — partially generated instances of Generable types — that drive SwiftUI updates directly. The `@Generable` macro auto-generates a `.PartiallyGenerated` type with all properties as Optional, enabling progressive UI rendering as the model generates each field.

### Other integration surfaces

**Writing Tools** work automatically with standard system text controls (UITextView, TextEditor, NSTextView) with no code changes. Developers customize behavior via `.writingToolsBehavior(.complete | .limited | .disabled)` but cannot programmatically trigger summarization or rewriting — these are user-initiated only. **App Intents** connect app actions to Siri and Spotlight; Apple Intelligence uses the on-device model to understand natural language requests and map them to registered intents. **SiriKit** remains supported for legacy domain-specific intents but App Intents is the recommended modern framework. A **content tagging adapter** provides first-class support for tag generation, entity extraction, and topic detection via `SystemLanguageModel(useCase: .contentTagging)`.

---

## How Apple engineers prompt the on-device model

Apple's internal prompt format uses structured special tokens for role delineation, discovered in macOS system files and confirmed through prompt extraction research. The format follows a chat template:

```
{{ specialToken.chat.role.system }}[System instructions]{{ specialToken.chat.component.turnEnd }}
{{ specialToken.chat.role.user }}[User content]{{ specialToken.chat.component.turnEnd }}
{{ specialToken.chat.role.assistant }}[Model generates here]
```

These render as tokens like `system‹n›`, `user‹n›`, `assistant‹n›`, and `‹turn_end›`. Each Apple Intelligence feature has a dedicated prompt template stored as a named configuration entry. For example, the mail reply template instructs: *"You are an assistant which helps the user respond to their mails... Please write a concise and natural reply... Please limit the reply within 50 words. Do not hallucinate. Do not make up factual information."* The Smart Reply system uses a two-stage prompt pipeline: first extracting relevant questions from the email with possible answer options (output as JSON), then generating the reply incorporating selected answers.

For notification summarization, the template designates the model as "an expert at summarizing messages" and wraps the dialogue content in `[Dialogue]...[End of Dialogue]` delimiters. The Photos memory creation prompt adopts a creative persona: *"You are a director on a movie set!"* and dynamically injects variables for story title, traits, target asset count, and chapter context.

**Prompt best practices** documented in WWDC 2025 sessions include keeping prompts concise (every token adds latency), writing instructions in English even when output is in another language, using `@Generable` and `@Guide` annotations rather than manual format instructions, ordering properties so dependent fields come after their dependencies, and using explicit length constraints like "in a few words" or "in a single paragraph." Apple's own prompts frequently include explicit safety instructions ("Do not hallucinate") and use the instruction hierarchy where system instructions take precedence over user prompts.

---

## Performance on real hardware and how to optimize for it

On iPhone 15 Pro, the 2024 model achieved approximately **0.6 milliseconds per prompt token** for time-to-first-token and **30 tokens per second** generation throughput before speculative decoding. With the draft model's 2–4× speedup, effective throughput reaches an estimated 60–90 tokens per second. The 2025 model's KV-cache sharing reduces TTFT by approximately 37.5%, and the move to 2-bit QAT further reduces memory bandwidth requirements.

The **Apple Neural Engine (ANE)** serves as the primary inference accelerator, described by Apple as an "energy-efficient and high-throughput engine for ML inference." The ANE handles the most demanding compute while freeing CPU and GPU for other tasks. Apple's implementation uses optimized 4D tensor formats (B, C, 1, S) and custom KV-cache update routines specifically designed for ANE data paths, achieving up to **10× speed and 14× less memory** compared to unoptimized deployments.

For developers optimizing their use of the model, several strategies matter. **Context window management** is critical given the 4,096-token hard limit: Apple recommends summarizing the session transcript at ~70% capacity and carrying only instructions plus the most recent exchange to new sessions. The `session.prewarm()` API prepares the model before first inference, reducing cold-start latency. Background apps face a system-allocated token budget — exceeding it triggers rate-limiting errors — while foreground apps have no rate limit unless the device is under heavy load. Battery impact is minimized by the ANE's energy efficiency and low-bit quantization reducing memory bandwidth, though Apple has not disclosed specific mAh figures. The system dynamically balances performance, battery life, and thermal conditions, potentially throttling throughput under sustained load.

---

## What works on-device and what gets routed elsewhere

Apple Intelligence uses a **three-tier routing architecture**. The on-device model (Tier 1) handles the majority of interactions by default: notification summarization, text rewriting and proofreading, Smart Reply, entity extraction, content classification, Genmoji creation, and basic text generation. These tasks work entirely offline with zero network requests. When requests exceed on-device capabilities — longer documents, complex cross-app reasoning, computationally intensive tasks — the system routes to **Private Cloud Compute** (Tier 2), running larger models on Apple Silicon servers with cryptographic verification and no data retention. **Third-party models** like ChatGPT (Tier 3) activate only with explicit user opt-in for broad world-knowledge queries.

Tasks that work well on-device include short text generation, grammar correction, notification and email summarization, Smart Reply, tone adjustment, entity extraction, content classification, and any task involving personal or private data. Tasks that should route to the server include long-document summarization, complex reasoning, multi-step logic, queries requiring current events or broad world knowledge, and anything exceeding the 4,096-token context window. The on-device model's **3B parameter scale inherently limits** its reasoning depth compared to larger server models — Apple's own documentation notes that "device-scale models require tasks to be broken down into smaller pieces."

Recommended developer use cases include personalized search suggestions, content tagging with the built-in adapter, structured data extraction from user input, dynamic NPC dialogue in games, form generation, and domain-specific AI assistants using tool calling. Developers can also train custom LoRA adapters for specialized datasets using Apple's Python toolkit.

---

## Hard constraints developers must design around

**Device requirements** form the primary gate. The on-device model requires an **A17 Pro or later** chip on iPhone (iPhone 15 Pro/Pro Max, all iPhone 16 models, iPhone 16e, iPhone 17 series) or any **M1 or later** chip on iPad or Mac. The critical hardware threshold is **8 GB RAM** — all supported devices meet this minimum. Storage requires **7 GB free** for the model files, which download automatically after a software update. No Intel Macs are supported.

The **4,096-token context window** (combined input and output) is the most significant developer constraint. The framework throws `GenerationError.exceededContextWindowSize` rather than silently truncating. Token density varies by language: English averages 3–4 characters per token, while CJK languages consume roughly 1 character per token, making the effective context window significantly smaller for non-Latin scripts. Apple does not expose a public tokenizer API — tokenization happens internally.

**Language support** spans 16 languages as of iOS 26.1: English (multiple regions), Chinese (Simplified and Traditional), Danish, Dutch, French, German, Italian, Japanese, Korean, Norwegian, Portuguese, Spanish, Swedish, Turkish, and Vietnamese. Apple Intelligence is **not available in mainland China** (devices purchased there or accounts set to that region), pending regulatory approval for a separate China-specific implementation reportedly based on Alibaba's Qwen models.

**Safety guardrails** operate in two layers. The model's alignment training makes it refuse harmful content generation, while a separate guardrail layer actively scans inputs and outputs, throwing `guardrailViolation` errors when content fails safety checks. Independent testing by CyCraft (June 2025) measured **99.5% jailbreak resistance** — the highest among comparable ~3B models — and 75.9% prompt extraction resistance, also best in class.

The Foundation Models framework requires **iOS 26 / iPadOS 26 / macOS Tahoe 26** minimum. Custom adapter development requires an Apple silicon Mac with 32 GB RAM (or a Linux GPU machine), a Foundation Models Framework Adapter Entitlement from the Apple Developer Program, and awareness that adapters must be retrained for each OS/model version update. Apple recommends maintaining "eval sets" of golden prompt/response pairs to validate behavior across model updates.

---

## Conclusion

Apple's on-device foundation model represents a distinctive engineering approach: rather than competing on raw parameter count, it achieves competitive quality through architectural innovations (dual-block KV-cache sharing, interleaved attention), extreme compression (2-bit QAT), and task-specific LoRA adapters that transform a single frozen base model into a family of specialists. The Foundation Models framework makes this infrastructure directly available to developers for the first time, with Swift-native typed generation that eliminates the parsing fragility typical of LLM integrations.

Three constraints shape every design decision around this model. The **4,096-token API context window** forces developers to manage context aggressively — summarizing transcripts, chunking inputs, and designing for concise interactions. The **3B parameter scale** limits reasoning depth, making the model excellent for extraction, classification, and short generation but poorly suited for complex multi-step logic. And the **version coupling** of adapters to specific OS releases creates an ongoing maintenance burden for developers training custom adapters. The model's strengths — privacy, offline capability, zero cost, low latency, and strong safety properties — make it ideal for tasks involving personal data, real-time UI feedback, and structured extraction, while complex reasoning and broad knowledge queries should route to server-side or third-party models.
